{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic inference\n",
    "\n",
    "Before we dive in to the Baysian nets let us have a small recap on the terms and some basics of Probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basis of Probability:\n",
    "\n",
    "#### Axioms:\n",
    "\n",
    "Probablity always lies between   0 and 1.\n",
    "* $ 0 \\leq P(A) \\leq 1 $\n",
    "\n",
    "Probability of \"True\" is 1 and Probability of \"False\" is 0.\n",
    "* $ P(true) = 1$ and  $ P(false) = 0$\n",
    "\n",
    "* $ P(A) + P(B) - P(A, B) = P(A \\bigvee B) $  \n",
    "$P(A \\bigvee B) \\rightarrow$ Probability of A OR B\n",
    "\n",
    "\n",
    "#### Definitions:\n",
    "\n",
    "$ P(a \\mid b) = \\frac{P(a, b)}{P(b)}$\n",
    "\n",
    "Probability of 'a, b' is equal to Probability of 'a' given 'b' times the Probability of 'b'\n",
    "\n",
    "$\\Rightarrow  P(a,b) = P(a \\mid b) \\cdotp P(b)$ \n",
    "\n",
    "\n",
    "\n",
    "#### Chain Rule:\n",
    "\n",
    "$P(a,b,c) = ?$\n",
    "\n",
    "let 'b,c' = y \n",
    "\n",
    "$\\Rightarrow  P(a,y) = P(a \\mid y) \\cdotp P(y) = P(a \\mid b,c) \\cdotp P(b,c)$\n",
    "$ = P(a \\mid b,c)\\cdotp P(b\\mid c) \\cdotp P(c) $\n",
    "\n",
    "In general\n",
    "\n",
    "$P(x_1,x_2,....x_n) = \\prod_{i=n}^1 P(x_i \\mid x_{i-1}...x_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Independence:\n",
    "\n",
    "If 'a' is independent of 'b' then, probability of 'a' given 'b' is nothing but probability of 'a'.\n",
    "\n",
    "Notation: $P(a\\mid b) = P(a)$\n",
    "\n",
    "#### Conditional Independence:\n",
    "Probablity 'a' given 'b,z' if 'a' is independent of 'b' then, probability of 'a' given 'b,z' is nothing but probability of 'a' given 'z'.\n",
    "\n",
    "Notation: $P(a\\mid b,z) = P(a \\mid z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Networks:\n",
    "\n",
    "Let us try to understand what is Baysian networks or Bayse Nets through one small example.\n",
    "\n",
    "Say we have a dog (D), Burgler (B), Raccon(R), Thrash can (T) and a Cop(C).\n",
    "\n",
    "Dog will bark when ever there is Burgler near the vicinity of the house  or a Raccon making it's daily visit to the house. Raccoon has the tendensy to make some noise, say it jumps in to the Thrash can in search of food and causes the thrash can to flip over etc, this noise annoys the dog and hence dog barks. Whenever dog barks too much, neighbours get irritated and call the cops. \n",
    "When cops arrive at the scene, Cops are unaware about the reason, why dog is barking. \n",
    "\n",
    "This analogy of example can be depicted as shown in the figure below.\n",
    "\n",
    "![story](ProbablisticInference.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above figure depicts the Baysian net. \n",
    "\n",
    "**NOTE:Baysian nets are Acycli Direct Graphs.**\n",
    "\n",
    "\n",
    "Here are some Probabilistic inferences on the above example:\n",
    "\n",
    "* Burgler and Raccoon are two independent activities, and doesn't have any parent. In other words, Raccoon doesn't wait for the Burgler to come and visit the house or Burgler doesn't wait for Raccoon to make its daily visit. Their prescense near the vicinity of the house is **Independent** of each other.\n",
    "* Dog has two parents, Burgler and Raccoon. i.e Dog barking depends on, either Burgler is in the vicinity or Raccoon making noise or both.\n",
    "* Cop has one parent, i.e. Dog. Cop is un-aware of the cause of Dog's barking. So Burgler and Raccoon are independent varialbes for Cop.\n",
    "* Trash can has one parent, i.e Raccoon. When ever Thrash can flips over, it is because of Raccoon.\n",
    "\n",
    "To make the probablistic inference on the above Baysian net, we will chew up the variables bottom up.\n",
    "\n",
    "**Analogy 1:**\n",
    "Cop --> Dog --> Burgler --> Thrashcan --> Raccoon\n",
    "\n",
    "OR \n",
    "\n",
    "**Analogy 2:**\n",
    "Cop --> Dog  --> Thrashcan --> Raccoon --> Burgler.\n",
    "\n",
    "The second analogy doesn't sound good. Because Dog has nothing to do with Thrash can. Dog will not know if the Thrashcan noise is because of Raccoon or some stray cat toppling it over.  So we stick with the analogy-1. As you can see sticking with one analogy which helps us understand and connect with our solution to the problem better. In other words, we arrange the variables in most relavent order careing only about its ascendents (bottom-up) as follows.\n",
    "\n",
    "$P(C,D,B,T,R)$\n",
    "\n",
    "From Chain rule,\n",
    "Probability of Cop arriving at the scene given the elements D,B,T and R is,\n",
    "\n",
    "$P(C,D,B,T,R) = P(C\\mid D,B,T,R) \\cdotp P(D \\mid B,T,R) \\cdotp P(B\\mid T,R) \\cdotp P(T\\mid R) \\cdotp P(R) \\rightarrow equation-1$\n",
    "\n",
    "\n",
    "Now, if we traverse the Baysian net tree from bottom up, we see that Cop is only dependent on Dog (Cop's parent node). In other words Cop is **independent** of B,T and R.\n",
    "therefore $ P(C\\mid D,B,T,R) = P(C\\mid D)$\n",
    "\n",
    "Similarly Dog is dependent only on Burgler and Raccoon (Dog's parent node). In other words Dog is **independent** of T.\n",
    "\n",
    "$P(D\\mid B,T,R) = P(D\\mid B,R)$\n",
    "\n",
    "Similarly Burgler and Raccoon doesn't have any parent node, therefore\n",
    "\n",
    "$P(B\\mid R,T) = P(B)$ and $P(R) = P(R)$\n",
    "\n",
    "Substituting these values in equation-1\n",
    "\n",
    "$P(C,D,B,T,R) = P(C\\mid D) \\cdotp P(D \\mid B,R) \\cdotp P(B) \\cdotp P(T\\mid R) \\cdotp P(R)$\n",
    "\n",
    "If we had to put all the the varialbles and created a probability table, that would have had a total rows of $2^{5} = 32$ rows in our Joint Probability table.\n",
    "\n",
    "Let us calcuate the number of rows required for the above Baysian net\n",
    "\n",
    "|net|rows|\n",
    "|--------|-----|\n",
    "|$P(C\\mid D)$|2|\n",
    "|$P(D \\mid B,R)$|4|\n",
    "|$P(B)$|1|\n",
    "|$P(T\\mid R)$|2\n",
    "|$P(R)$|1|\n",
    "|Total|10|\n",
    "\n",
    "The above table is called Conditional Probability Table.\n",
    "\n",
    "With the help of Baysian nets we are able to reduce the number of rows to 10. which is a significant improvement compared to  32.\n",
    "\n",
    "This may not seem much for now, but if you think of having millions of columns or features or charecters in your problem each with \n",
    "\n",
    "\n",
    "### Formal definition of Baysian nets:\n",
    "\n",
    "**Each variable is conditionally independent of its non-descendnts given its parent.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayse:\n",
    "\n",
    "$P(a\\mid b) = \\frac {P(a,b)} {P(b)}$\n",
    "\n",
    "\n",
    "$\\Rightarrow P(a\\mid b)\\cdotp P(b) = P(a,b) = P(b\\mid a)\\cdotp P(a)$\n",
    "\n",
    "$\\Rightarrow P(a\\mid b)\\cdotp P(b) = P(b\\mid a)\\cdotp P(a)$\n",
    "\n",
    "$\\Rightarrow P(a\\mid b) = \\frac {P(b\\mid a)\\cdotp P(a)}{P(b)}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
